Abstract:
End-to-end speech recognition has emerged as a transformative approach in the field of automatic speech recognition (ASR), replacing traditional pipelines that relied on separate acoustic, language, and pronunciation models. This approach simplifies model architecture by integrating all components into a unified deep learning framework, typically using recurrent neural networks (RNNs), convolutional neural networks (CNNs), or transformer-based architectures. In this work, we explore an end-to-end speech recognition system trained directly on raw audio data and corresponding transcriptions, leveraging models such as Connectionist Temporal Classification (CTC), attention-based encoder-decoder frameworks, and Transformer models like Whisper. Our implementation demonstrates competitive performance on benchmark datasets, reducing error rates while simplifying deployment. We discuss training strategies, data preprocessing, model architecture, and evaluate the system on real-world speech inputs. The results confirm that end-to-end systems can achieve high accuracy, adaptability, and scalability, making them suitable for a wide range of speech-enabled applications.
